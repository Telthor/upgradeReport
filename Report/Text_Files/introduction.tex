% !TEX root = ../Thesis.tex

\chapter{Introduction}

Quantum computing has been an active field of research since the concept was first suggested by Richard Feynman in the early 1980s \cite{Feynman1982}.
Originally proposed as an efficient method for simulating chemical processes (something that traditional computers find extremely taxing) the discovery of several algorithms offering significant speed increases over classical computers has further fueLled research \cite{Shor1994,Shor1997}.
Strong initial scepticism abounded regarding the potential for quantum computers to exist in the real world, primarily due to the concerns that error correction with such a complex device would be impossible\cite{Preskill1997}.
This pessimism gave way with the identification of an error threshold for quantum computation.
Given an error rate below a critical threshold it was possible to perform an arbitrarily long computation with negligible possibility of significant error \cite{Shor1996,Aharonov1997}.
Following these discoveries serious attention has been given to the development of error correcting codes that might be implemented to allow a physical quantum computer to be \emph{fault tolerant}.
\\
Gottesman identified the class of error correcting codes known as \emph{stabilizer codes}, where codes are defined by the group of logical operators that leave the code unchanged \cite{Gottesman1997a}.
This class of quantum error correcting codes have become the dominant form in theoretical research.
Of these one in particular has become the focus of much of the ongoing research of quantum computing, \emph{the surface code}, developed by Bravyi and Kitaev \cite{Bravyi1998}.
Although other quantum error correcting codes (such as colour codes \cite{Vasmer2018}) exist, the surface code has become the focus for experimental implementations. This is due mainly to the relatively high error threshold that it is able to tolerate, $\sim0.5\%$, and the simple architecture of a planar grid of qubits.
\\
In recent years there has been rapid progress in the development of physical qubits.
Groups in both the academic and private sectors have shown small numbers of qubits functioning with error rates above the fault tolerant threshold \cite{Barends2015,Reagor2017}.
The successful recent approaches have tended to focus on superconducting circuits to produce their qubits.
Whilst these have proved excellent for the small numbers of qubits currently in use, it is likely that they will present significant additional challenges when scaling to numbers sufficient for useful, fault-tolerant quantum computation.
With the numbers required likely to be close to $100 \times 10^6$ and the current size of these qubits close to 1mm$^2$, it will likely be impossible to use these qubits in their current form in a fault-tolerant quantum computer.
\\
Although there are many alternative systems that could provide a qubit, this paper will focus on the use of the spins of nuclei and electrons bound to donors in semiconductors, particularly silicon.
A seminal paper by Kane \cite{Kane1998} stimulated much of the research interest in this field.
He proposed using the spin of phosphorus nuclei in silicon as qubits with the ability to mediate interactions between neighbouring donor nuclei using the interaction between the electrons bound to each.
These types of qubit are attractive due to their exceptionally long coherence times, the time that the qubit reliably stores quantum information for. 
A long coherence time relative to qubit gate times is essential, as this determines the error rate of the qubits.
Coherence times as long as several seconds have been reported for donor spin qubits in silicon, whilst gate times can be as low as several nanoseconds \cite{Wolfowicz2013}. 
Despite these advantages, development of these types of qubits for quantum computers has lagged behind the superconducting and ion trap versions \cite{Ballance2015}. 
This is due to the difficulty of isolating and addressing single donors in a silicon lattice. 
Kane's proposal required sub nano-metre precision in qubit placement to facilitate inter-qubit interactions. 
Even if this precision were achieved there remains the issue of how the requisite control circuitry could be integrated into such a dense design.
This has lead to the development of more modern proposals to both overcome these difficulties and also to implement surface code based error correction.
One such proposal is from O'Gormann et al \cite{OGorman2014}. 
This proposal takes a similar approach to Kane, with qubits being donor spins in a silicon lattice. 
Where it differs significantly is in its use of two lattices of qubits. 
One of these is for the storage of data, whilst the other performs measurements on these data qubits. 


\begin{equation}
\hat{H} = \mu_bBg_eS_z + \mu_ng_nBI_z + A \hat{S}\cdot\hat{I}
\end{equation}


